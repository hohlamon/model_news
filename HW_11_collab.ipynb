{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW 11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ДЗ 11: сравниваем ассоциаты одного и того же слова в разных корпусах"
      ],
      "metadata": {
        "id": "bUWR8_P1Nt72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание: попробовать обучить две векторные модели на двух различных корпусах, которые можно сравнить между собой. \n",
        "\n",
        "Например: \n",
        "\n",
        "*   тексты газет из 1900 года (первая модель) и тексты газет из наших дней (вторая модель)\n",
        "*   собрание сочинений одного писателй и собрание сочинений другого писателя (писатели должны быть сопоставимы: одно время работы, один тип текстов, один язык)\n",
        "\n",
        "Найдите не меньше 3 слов, которые имеют ощутимо различные наборы семантических ассоциатов в двух моделях. Постарайтесь найти такие слова, различия между которыми могут быть проинтерпретированы и могут что-то говорить о разнице корпусов. \n",
        "\n",
        "**Важно: размер каждого корпуса должен быть не меньше 20 МБ.**"
      ],
      "metadata": {
        "id": "8Jk09Mvv1OhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Загружаем корпус 1:"
      ],
      "metadata": {
        "id": "k4p_9rVIHxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/hohlamon/model_news/raw/main/meduza_2.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4DF-ddZf4Sw",
        "outputId": "ccfa1bed-fe2d-4257-d7fe-0eb7a8ffd4b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-03 10:32:17--  https://github.com/hohlamon/model_news/raw/main/meduza_2.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hohlamon/model_news/main/meduza_2.txt [following]\n",
            "--2022-05-03 10:32:17--  https://raw.githubusercontent.com/hohlamon/model_news/main/meduza_2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76710904 (73M) [text/plain]\n",
            "Saving to: ‘meduza_2.txt.2’\n",
            "\n",
            "meduza_2.txt.2      100%[===================>]  73.16M   175MB/s    in 0.4s    \n",
            "\n",
            "2022-05-03 10:32:18 (175 MB/s) - ‘meduza_2.txt.2’ saved [76710904/76710904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/hohlamon/model_news/raw/main/rt_2.txt"
      ],
      "metadata": {
        "id": "HKUOkVLKF5N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import spacy\n",
        "import gensim"
      ],
      "metadata": {
        "id": "hmgn3_HezWXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c56d89-6275-42cc-941a-df741db62e06"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.2"
      ],
      "metadata": {
        "id": "kLWVic8qHtwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n"
      ],
      "metadata": {
        "id": "pEkN6grqzJoN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_md"
      ],
      "metadata": {
        "id": "RmAA5D44SYdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "russian_model = spacy.load('ru_core_news_md', disable=['ner', 'attribute_ruler'])"
      ],
      "metadata": {
        "id": "uv7EMNFKTb8r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'rt_2.txt' # свой файл надо положить рядом и прописать тут путь\n",
        "## выгрузим текст из файла\n",
        "with open(filename, encoding='utf-8') as open_file:\n",
        "    text_as_lines = open_file.read()"
      ],
      "metadata": {
        "id": "k5-cGk2Bmehy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_text = sent_tokenize(text_as_lines)"
      ],
      "metadata": {
        "id": "Gm_vLOymgelv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Предобрабатываем \n",
        "\n",
        "(не обязательно использовать именно spacy)"
      ],
      "metadata": {
        "id": "_HRT_YFtOH2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Достаточно большой корпус для векторной модели может и обрабатываться достаточно долго. Например, на 14 русских романов у меня ушло минут 20 здесь."
      ],
      "metadata": {
        "id": "RJOi6Flqq-R5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Впрочем, вы можете подумать о том, как оптимизировать предобработку. Или не думать -- и просто дать компьютеру поработать часок-другой. "
      ],
      "metadata": {
        "id": "BGAJhdjf09M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def poser (sent):\n",
        "  output = []\n",
        "  parsed = russian_model(sent)\n",
        "  for word in parsed:\n",
        "    output.append(word.lemma_+'_'+word.pos_)\n",
        "  return output"
      ],
      "metadata": {
        "id": "YgQxv6qBgtS-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "procesed_text = [poser(sent) for sent in sent_text]"
      ],
      "metadata": {
        "id": "Qk_jVPFcgulF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'meduza_2.txt' # свой файл надо положить рядом и прописать тут путь\n",
        "## выгрузим текст из файла\n",
        "with open(filename, encoding='utf-8') as open_file:\n",
        "    text_as_lines = open_file.read()"
      ],
      "metadata": {
        "id": "d6gTyskyGJvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_text_2 = sent_tokenize(text_as_lines)"
      ],
      "metadata": {
        "id": "Io6tO25NGUZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "procesed_text_2 = [poser(sent) for sent in sent_text]"
      ],
      "metadata": {
        "id": "E8gDieNSGUOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Такое можно подавать в word2vec"
      ],
      "metadata": {
        "id": "RKcjtnVz2mAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Обучаем на этом модель"
      ],
      "metadata": {
        "id": "gwWMPe4HuM4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "vPnD5cXGavDG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "YAUJtstQzFtu"
      },
      "outputs": [],
      "source": [
        "modelRT = gensim.models.Word2Vec(procesed_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save(\"model_rt.wordvectors\")\n"
      ],
      "metadata": {
        "id": "JoGHPnCU9dCO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelMED = gensim.models.Word2Vec(procesed_text_2)"
      ],
      "metadata": {
        "id": "gwn7eR6RG5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model_med.wordvectors\")"
      ],
      "metadata": {
        "id": "jsutweAeG-YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь моделью можно пользоваться:"
      ],
      "metadata": {
        "id": "lsVbIoEuuWEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelRT.wv.most_similar('оппозиционер_NOUN', topn = 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gox-anKyJ2s",
        "outputId": "4f323fcd-4a73-4c80-dc31-e4bf54d9e365"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('повстанец_NOUN', 0.7110620737075806),\n",
              " ('мусульманин_NOUN', 0.6926690936088562),\n",
              " ('мошенник_NOUN', 0.6732779145240784),\n",
              " ('либерал_NOUN', 0.6621700525283813),\n",
              " ('хуситы_NOUN', 0.6608474254608154),\n",
              " ('хуситов_NOUN', 0.660496175289154),\n",
              " ('радикал_NOUN', 0.6563068628311157),\n",
              " ('похититель_NOUN', 0.651795506477356),\n",
              " ('шиит_NOUN', 0.6398146748542786),\n",
              " ('оппозиция_NOUN', 0.6315505504608154),\n",
              " ('экстремист_NOUN', 0.6302471160888672),\n",
              " ('ополченец_NOUN', 0.6236810088157654),\n",
              " ('багдади_NOUN', 0.6232008934020996),\n",
              " ('одномандатник_NOUN', 0.621712863445282),\n",
              " ('социал_NOUN', 0.6202377676963806),\n",
              " ('аль_NOUN', 0.617321252822876),\n",
              " ('трансгендерами_NOUN', 0.6142008304595947),\n",
              " ('националист_NOUN', 0.6110759973526001),\n",
              " ('ккк_PROPN', 0.6084008812904358),\n",
              " ('хуситами_NOUN', 0.607941210269928)]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelMED.wv.most_similar('оппозиционер_NOUN', topn = 20)"
      ],
      "metadata": {
        "id": "4gaoFa_HHJFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Теперь проделываем то же самое для другого корпуса — и сравниваем слова в двух моделях. Описываем результаты и интерпретируем их здесь же в тетрадке."
      ],
      "metadata": {
        "id": "T-pGtUMJaeNb"
      }
    }
  ]
}